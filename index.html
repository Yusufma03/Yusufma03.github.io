<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiao Ma's Homepage</title>
  
  <meta name="author" content="Xiao Ma">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiao Ma (马骁)</name>
              </p>
              <p>I am a research scientist at <a href="https://sail.sea.com/">SEA AI Lab</a>. I pursued my PhD at National University of Singapore, advised by <a href="https://www.comp.nus.edu.sg/~dyhsu/">Prof. David Hsu</a>. I also worked closely with <a href="https://www.comp.nus.edu.sg/~leews/">Prof. Wee Sun Lee</a> on my projects. I received my B.Sc. in Computer Science from Shanghai Jiao Tong University in 2017, where I was advised by <a href="http://www.cs.sjtu.edu.cn/~fwu/">Prof. Fan Wu</a> and <a href="http://www.cs.sjtu.edu.cn/~gao-xf/">Prof. Xiaofeng Gao</a>.
              </p>
              <p>
                My research focuses on uncertainty modelling, reinforcement learning, graph neural networks, and their applications to robotics.
              </p>
              <p style="color:red">We're hiring research scientists and interns! Please reach out if you are interested in working with us :)</p>
              <p style="text-align:center">
                <a href="mailto:max@sea.com">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.sg/citations?user=hR4G6hoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Xiao-Ma/145572784">Semantic Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/yusufma555">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/Yusufma03/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>- [Feb. 2022] G-DOOM for deformable object manipulation has been accepted to ICRA 2022!</p>
            <p>- [Nov. 2021] I've successfully defensed my PhD thesis!</p>
            <p>- [Oct. 2021] SABRA for object relationship detection under imbalanced distributions has been accepted to BMVC 2021.</p>
            <p>- [Jul. 2021] I'm joining <strong>SEA AI Lab</strong> as a research scientist working on reinforcement learning.</p>
            <p>- [May 2021] PROMPT for ab-initio object manipulation has been accepted by RSS 2021.</p>
            <p>- [Oct. 2020] CVRL for model-based RL under complex observations has been accepted by CoRL 2020.</p>
            <p>- [Sept. 2020] BALMS for long-tailed visual recognition has been accepted by NeurIPS 2020.</p>
            <p>- [Jul. 2020] STAR for pedestrian trajectory prediction has been accepted by ECCV 2020.</p>
            <p>- [Dec. 2019]  DPFRL for reinforcement learning under complex and partial observations has been accepted by ICLR 2020.</p>
            <p>- [Nov. 2019] PF-RNNs for sequence modeling under uncertainty has been accepted to AAAI 2020.</p>
            <p>- [Jun. 2019] DAN was nominated for the <strong>best system paper and best student paper</strong> of RSS 2019!</p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications <a href="https://scholar.google.com.sg/citations?user=hR4G6hoAAAAJ&hl=en">(google scholar)</a></heading>
            </td>
          </tr>
          
        </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
            <td style="padding:20px;width:25%;vertical-align:left">
              <div class="one">
                <img src='data/ma2021gdoom/gdoom.png' width="140">
              </div>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/g-doom/">
              <papertitle>Learning Latent Graph Dynamics for Deformable Object Manipulation</papertitle>
              </a>
              <br>
              <strong>Xiao Ma</strong>,
              <a href="https://www.comp.nus.edu.sg/~dyhsu/">David Hsu</a>,
              <a href="https://www.comp.nus.edu.sg/~leews/">Wee Sun Lee</a>,
              <br>
							<em>International Conference on Robotics and Automation (ICRA)</em>, 2022 &nbsp <font color="red"></font>
              <br>
              <a href="https://sites.google.com/view/g-doom/">project page</a>
              /
              <a href="https://arxiv.org/abs/2104.12149">pdf</a>
              /
              <a href="data/ma2021gdoom/ma2021gdoom.bib">bibtex</a>
              <p>We present G-DOOM for deformable object manipulation. G-DOOM abstract an deformable object as a keypoint-based graph and models the spatio-temporal keypoint interactions with Recurrent Graph Dynamics. G-DOOM achieves SOTA performance on a set of deformable object manipulation tasks.</p>
            </td>      

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <td style="padding:20px;width:25%;vertical-align:left">
              <div class="one">
                <img src='data/chen2021prompt/prompt.png' width="140">
              </div>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://adacomp.comp.nus.edu.sg/2021/06/26/particle-based-robot-manipulation/">
              <papertitle>Ab Initio Particle-based Object Manipulation</papertitle>
              </a>
              <br>
              Siwei Chen,
              <strong>Xiao Ma</strong>,
              <a href="https://www.comp.nus.edu.sg/~dyhsu/">David Hsu</a>,
              <br>
							<em>Robotics: Science and Systems (RSS)</em>, 2021 &nbsp <font color="red"></font>
              <br>
              <a href="https://adacomp.comp.nus.edu.sg/2021/06/26/particle-based-robot-manipulation/">project page</a>
              /
              <a href="https://arxiv.org/abs/2107.08865">pdf</a>
              /
              <a href="https://github.com/AdaCompNUS/Prompt">code</a>
              /
              <a href="data/chen2021prompt/chen2021prompt.bib">bibtex</a>
              <p>This paper introduces PROMPT, a framework for particle-based object manipulation. PROMPT performs high-quality online point cloud reconstruction from multi-view images captured by an eye-in-hand camera. It achieves high performance in object grasping, pushing, and placing.</p>
            </td>

 


            <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
              <td style="padding:20px;width:25%;vertical-align:left">
                <div class="one">
                  <img src='data/jin2021towards/sabra.png' width="140">
                </div>
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2012.12510">
                <papertitle>Towards Overcoming False Positives in Visual Relationship Detection</papertitle>
                </a>
                <br>
                Daisheng Jin*,
                <strong>Xiao Ma</strong>*,
                Chongzhi Zhang*,
                Yizhuo Zhou,
                Jiashu Tao,
                <a href="https://scholar.google.com/citations?user=2QLD4fAAAAAJ&hl=en">Mingyuan Zhang</a>,
                <a href="https://scholar.google.com/citations?user=sMQV1ecAAAAJ&hl=zh-CN">Haiyu Zhao</a>,
                <a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ&hl=zh-CN">Shuai Yi</a>,
                <a href="https://scholar.google.com.hk/citations?user=e-4LoEcAAAAJ&hl=zh-CN">Zhoujun Li</a>,
                <a href="https://scholar.google.com/citations?user=8VY7ZDcAAAAJ&hl=en">Xianglong Liu</a>,
                <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                (* equal contribution)
                <br>
                <em>ArXiv Preprint 2012.12510</em>, 2021 &nbsp <font color="red"></font>
                <br>
                <a href="https://arxiv.org/abs/2012.12510">pdf</a>
                /
                <a href="data/jin2021towards/jin2021towards.bib">bibtex</a>
                <p>We observe that one major cause to high false positive rate in visual relationship detection (VRD) is its highly imbalanced negative proposal distribution. To tackle this issue, we introduce SABRA, a simple yet effective framework for balancing negative proposals in VRD. </p>
              </td> -->


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
            <td style="padding:20px;width:25%;vertical-align:left">
              <div class="one">
                <img src='data/ma2020contrastive/walker.gif' width="140">
              </div>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/cvrl/">
              <papertitle>Contrastive Variational Reinforcement Learning for Complex Observations</papertitle>
              </a>
              <br>
              <strong>Xiao Ma</strong>,
              Siwei Chen,
              <a href="https://www.comp.nus.edu.sg/~dyhsu/">David Hsu</a>,
              <a href="https://www.comp.nus.edu.sg/~leews/">Wee Sun Lee</a>,
              <br>
							<em>In Proceedings of The 4nd Conference on Robot Learning (CoRL)</em>, 2020 &nbsp <font color="red"></font>
              <br>
              <a href="https://sites.google.com/view/cvrl/">project page</a>
              /
              <a href="https://arxiv.org/abs/2008.02430">pdf</a>
              /
              <a href="https://github.com/Yusufma03/CVRL">code</a>
              /
              <a href="https://www.youtube.com/watch?v=koXGdHR6Nd4">talk</a>
              /
              <a href="data/ma2020contrastive/ma2020contrastive.bib">bibtex</a>
              <p>We introduce CVRL, contrastive model-based reinforcement learning for complex observations. Different from standard generative models, CVRL learns a <em>contrastive</em> latent world model and significantly improves the robustness against complex observations.</p>
            </td>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
              <td style="padding:20px;width:25%;vertical-align:left">
                <div class="one">
                  <img src='data/ren2020balanced/balms.png' width="140" height="140">
                </div>
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2007.10740">
                <papertitle>Balanced Meta-Softmax for Long-Tailed Visual Recognition</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com.sg/citations?user=YUKPVCoAAAAJ&hl=en">Jiawei Ren</a>
                <a href="http://cjyu.me/">Cunjun Yu</a>,
                Shunan Sheng,
                <strong>Xiao Ma</strong>,
                <a href="https://scholar.google.com/citations?user=sMQV1ecAAAAJ&hl=zh-CN">Haiyu Zhao</a>,
                <a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ&hl=zh-CN">Shuai Yi</a>,
                <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                <br>
                <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2020 &nbsp <font color="red"></font>
                <br>
                <a href="https://arxiv.org/abs/2007.10740">pdf</a>
                /
                <a href="https://github.com/jiawei-ren/BalancedMetaSoftmax">code</a>
                /
                <a href="data/ren2020balanced/ren2020balanced.bib">bibtex</a>
                <p>Our key observation is that softmax is biased under the long-tailed distribution. BALMS provides a mathematically unbiased gradient estimate for long-tailed distributions and applies meta-learning to further improve the data sampling process.</p>
              </td>

              <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <td style="padding:20px;width:25%;vertical-align:left">
                <div class="one">
                  <img src='data/chen2020diner/diner.png' width="140">
                </div>
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2007.06207">
                <papertitle>DinerDash Gym: A Benchmark for Policy Learning in High-Dimensional Action Space</papertitle>
                </a>
                <br>
                Siwei Chen,
                <strong>Xiao Ma</strong>,
                <a href="https://www.comp.nus.edu.sg/~dyhsu/">David Hsu</a>,
                <br>
                <em>In IL workshop, Robotics: Science and Systems (RSS)</em>, 2020 &nbsp <font color="red"></font>
                <br>
                <a href="https://arxiv.org/abs/2007.06207">pdf</a>
                /
                <a href="https://github.com/AdaCompNUS/diner-dash-simulator">code</a>
                /
                <a href="data/chen2020diner/chen2020diner.bib">bibtex</a>
                <p>We present DinerDash Gym, a light-weight benchmark for policy learning with high-dimensional action space. </p>
              </td> -->


              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
                <td style="padding:20px;width:25%;vertical-align:left">
                  <div class="one">
                    <img src='data/yu2020spatio/star.png' width="140" height="140">
                  </div>
                </td>
                <td style="padding:10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2005.08514">
                  <papertitle>Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction</papertitle>
                  </a>
                  <br>
                  <a href="http://cjyu.me/">Cunjun Yu*</a>,
                  <strong>Xiao Ma*</strong>,
                  <a href="https://scholar.google.com.sg/citations?user=YUKPVCoAAAAJ&hl=en">Jiawei Ren</a>,
                  <a href="https://scholar.google.com/citations?user=sMQV1ecAAAAJ&hl=zh-CN">Haiyu Zhao</a>,
                  <a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ&hl=zh-CN">Shuai Yi</a>
                  (* equal contribution)
                  <br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2020 &nbsp <font color="red"></font>
                  <br>
                  <a href="https://sites.google.com/view/star-eccv2020/home">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2005.08514">pdf</a>
                  /
                  <a href="https://github.com/Majiker/STAR">code</a>
                  /
                  <a href="https://www.youtube.com/watch?v=5tS5Xe-DERo">talk</a>
                  /
                  <a href="data/yu2020spatio/yu2020spatio.bib">bibtex</a>
                  <p>We introduce STAR, the first transformer-based pedestrian trajectory predictor. STAR generalizes the Transformers into spatio-temporal graphs and significantly improves the trajectory prediction accuracy (2x).</p>
                </td>

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
                  <td style="padding:20px;width:25%;vertical-align:left">
                    <div class="one">
                      <img src='data/ma2020discriminative/dpfrl.gif' width="140" height="140"> 
                    </div>
                  </td>
                  <td style="padding:10px;width:75%;vertical-align:middle">
                    <a href="https://sites.google.com/view/dpfrl">
                    <papertitle>Discriminative Particle Filter Reinforcement Learning for Complex Partial Observations</papertitle>
                    </a>
                    <br>
                    <strong>Xiao Ma</strong>,
                    <a href="http://karkus.tilda.ws/">Peter Karkus</a>,
                    <a href="https://www.comp.nus.edu.sg/~dyhsu/">David Hsu</a>,
                    <a href="https://www.comp.nus.edu.sg/~leews/">Wee Sun Lee</a>,
                    <br>
                    <em>International Conference on Learning Representations (ICLR)</em>, 2020 &nbsp <font color="red"></font>
                    <br>
                    <a href="https://sites.google.com/view/dpfrl/">project page</a>
                    /
                    <a href="https://openreview.net/forum?id=HJl8_eHYvS">pdf</a>
                    /
                    <a href="https://github.com/Yusufma03/DPFRL">code</a>
                    /
                    <a href="https://iclr.cc/virtual_2020/poster_HJl8_eHYvS.html">talk</a>
                    /
                    <a href="data/ma2020discriminative/ma2020discriminative.bib">bibtex</a>
                    <p>We introduce DPFRL for reinforcement learning for complex partial observations. DPFRL encodes a <em>discriminative particle algorithm</em> as a differentiable computational graph in neural networks which improves the belief tracking.</p>
                  </td>

                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
                    <td style="padding:20px;width:25%;vertical-align:left">
                      <div class="one">
                        <img src='data/ma2020particle/pfrnn.png' width="140" height="140"> 
                      </div>
                    </td>
                    <td style="padding:10px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/1905.12885">
                      <papertitle>Particle Filter Recurrent Neural Networks</papertitle>
                      </a>
                      <br>
                      <strong>Xiao Ma*</strong>,
                      <a href="http://karkus.tilda.ws/">Peter Karkus*</a>,
                      <a href="https://www.comp.nus.edu.sg/~dyhsu/">David Hsu</a>,
                      <a href="https://www.comp.nus.edu.sg/~leews/">Wee Sun Lee</a>
                      (* equal contribution)
                      <br>
                      <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2020 &nbsp <font color="red"></font>
                      <br>
                      <a href="https://arxiv.org/abs/1905.12885">pdf</a>
                      /
                      <a href="https://github.com/Yusufma03/pfrnns">code</a>
                      /
                      <a href="data/ma2020particle/ma2020particle.bib">bibtex</a>
                      <p>We introduce PF-RNNs for general sequence prediction under uncertainty. PF-RNNs encodes a differentiable particle filter algorithm with standard RNNs and improves the general sequence prediction performance.</p>
                    </td>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
                      <td style="padding:20px;width:25%;vertical-align:left">
                        <div class="one">
                          <img src='data/karkus2019differentiable/dan.png' width="140" height="140"> 
                        </div>
                      </td>
                      <td style="padding:10px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/1905.11602">
                        <papertitle>Differentiable Algorithm Networks for Composable Robot Learning</papertitle>
                        </a>
                        <br>
                        <a href="http://karkus.tilda.ws/">Peter Karkus</a>,
                        <strong>Xiao Ma</strong>,
                        <a href="https://www.comp.nus.edu.sg/~dyhsu/">David Hsu</a>,
                        <a href="https://scholar.google.com/citations?user=IcasIiwAAAAJ&hl=en">Leslie Kaelbling</a>,
                        <a href="https://www.comp.nus.edu.sg/~leews/">Wee Sun Lee</a>
                        <a href="https://scholar.google.ca/citations?user=gQOKAggAAAAJ&hl=en">Tomas Lozano-Perez</a>
                        <br>
                        <em>Robotics: Science and Systems (RSS)</em>, 2019 &nbsp <font color="red">best system paper finalist & best student paper finalist</font>
                        <br>
                        <a href="https://arxiv.org/abs/1905.11602">pdf</a>
                        /
                        <a href="data/karkus2019differentiable/karkus2019dan.bib">bibtex</a>
                        <p>A DAN is composed of neural network modules, each encoding a differentiable algorithm and an associated model; and it is trained end-to-end from data. The algorithms and models act as structural priors to reduce the data requirements for learning; end-to-end learning allows the modules to adapt to one another and compensate for imperfect models and algorithms. </p>
                      </td>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Stolen from <a href="https://github.com/jonbarron/website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
